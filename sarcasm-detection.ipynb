{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Surestart Action Item Day 5\nThis notebook will train and test a basic neural network, using Keras, to determine whether a headline is sarcastic or not.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Necessary packages\nimport pandas as pd\nimport numpy as np\nimport os\nimport json\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import cross_val_score as cvs\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport keras\nimport tensorflow as tf\nfrom keras import models\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.losses import MeanSquaredError as mse\nimport sklearn.metrics\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Getting pathnames for each file in the input folder\nfor dirname, _, filenames in os.walk('/kaggle/input/news-headlines-dataset-for-sarcasm-detection'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Function to parse data\ndef parse_data(file):\n    for l in open(file,'r'):\n        yield json.loads(l)\n\n#Taking in the data in one of the json files (I'm using the slightly larger one)\ndata = list(parse_data(\"/kaggle/input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json\"))\ndata[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****We need to separate the data from this list into X and y****  \nOur X is going to be a vectorized list of words from the headline.  \nOur y is going to be \"is_sarcastic\".  \nTo do this, we're going to use some NLP packages  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating our X variable\nvectorizer = TfidfVectorizer(max_features=50, use_idf=False)\nheadlines = [i['headline'] for i in data]\nX = vectorizer.fit_transform(headlines).toarray()\n\n#Creating our y variable\ny = np.ravel([i['is_sarcastic'] for i in data])\n\n#Creating a train and test split\nX_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 1693)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we're going to build the model with its layers\n\n#Initialize the model\nmodel = Sequential()\n\n#Add the input layer\nmodel.add(Dense(24, activation = 'softmax', input_shape = (50,)))\n\n#Add first hidden layer\nmodel.add(Dense(12, activation = 'softmax'))\n\n#Add second hidden layer\nmodel.add(Dense(8, activation = 'softmax'))\n\n#Add output layer\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we're going to compile the model\n#Our loss function is binary crossentropy\n#Our optimizer is adam\n\nmodel.compile(loss = 'binary_crossentropy', \n              optimizer = 'adam',\n              metrics = ['accuracy', 'mse'])\n\n#We're going to also fit the model\n#We're going to do 20 epochs\n#The batch size will be 224 to get ~100 iterations per epoch\nmodel.fit(X_train, y_train, epochs = 20,\n          batch_size = 224, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Next, we'll test the model on the test dataset we set aside\n\n#Prediction on the X_test data, round each to an integer (either 0 or 1)\ny_pred = np.around(model.predict(X_test))\n\n#We're going to now look at the accuracy and loss\nscore = model.evaluate(X_test, y_test, verbose=1)\nprint(score)\n\n#We'll print precision and recall too\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we're going to make a confusion matri\n#The rows are the known labels, the columns are the predicted labels\nmatrix = cm(y_test, y_pred)\ndf = pd.DataFrame(columns = ['', 'is_sarcastic', 'not_sarcastic'])\ndf.loc[len(df)] = ['is_sarcastic', matrix[0][0], matrix[0][1]]\ndf.loc[len(df)] = ['not_sarcastic', matrix[1][0], matrix[1][1]]\nprint(df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}